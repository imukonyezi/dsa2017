{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import operator \n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14283, 15)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_pickle('cleaned_non_retweets.pkl')\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"@ntvuganda: Kizza Besigye has been arrested by Police in Naguru for unclear reasons #UgandaDecides https://t.co/n17GCGQHUc\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tweet = tweets['text'][0]\n",
    "sample_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing beautiful soup to clean the tweet texts\n",
    "from bs4 import BeautifulSoup\n",
    "# b_classifier = BeautifulSoup(sample_tweet)\n",
    "# print b_classifier.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "emoticons_str = r\"\"\"(?:\n",
    "        [:=;] \n",
    "        [oO\\-]? \n",
    "        [D\\)\\]\\(\\]/\\\\OpP] \n",
    "    )\"\"\"\n",
    "# Eyes\n",
    "# Nose (optional)\n",
    "# Mouth\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        #tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "        tokens = [token if tokens_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_tweet_ = preprocess(sample_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"',\n",
       " '@ntvuganda',\n",
       " ':',\n",
       " 'Kizza',\n",
       " 'Besigye',\n",
       " 'has',\n",
       " 'been',\n",
       " 'arrested',\n",
       " 'by',\n",
       " 'Police',\n",
       " 'in',\n",
       " 'Naguru',\n",
       " 'for',\n",
       " 'unclear',\n",
       " 'reasons',\n",
       " '#UgandaDecides',\n",
       " 'https://t.co/n17GCGQHUc',\n",
       " '\"']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tweet_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenization from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creatinga function for re-usability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweets_to_words(tweets,stem):\n",
    "    # 1. Remove HTML\n",
    "    tweets_text = BeautifulSoup(tweets, 'lxml').get_text() \n",
    "    # 2. Remove non-letters        \n",
    "    tweet_words = preprocess(tweets_text)\n",
    "    # 3. Convert to lower case, split into individual words  \n",
    "    tweets_lower = [tweet.lower() for tweet in tweet_words]\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words= [w for w in tweets_lower if not w in stops]\n",
    "    meaningful_words = [x.decode(\"utf-8\").encode('utf','ignore') for x in meaningful_words]\n",
    "    #\n",
    "    # 6. Doing Stemming or Lemmatization (Normalising the text)\n",
    "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "    if (stem=='S'):  # Choosing between Stemming ('S') and Lemmatization ('L')\n",
    "        stemmer=PorterStemmer()\n",
    "        final_words=[stemmer.stem(x) for x in meaningful_words]\n",
    "    else: \n",
    "        lemma=WordNetLemmatizer()\n",
    "        final_words=[lemma.lemmatize(x) for x in meaningful_words]\n",
    "    # 7. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join(final_words))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'\" @ntvuganda : kizza besigye arrested police naguru unclear reason #ugandadecides https://t.co/n17gcgqhuc \"'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_to_words(tweets['text'][0],\"L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_tweets = tweets['text'].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's loop through and clean all of the training set \n",
    "### at once (this might take a few minutes depending on your computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Cleaning and parsing the training set movie reviews...\\n\"\n",
    "clean_tweets = []\n",
    "for i in xrange(0, num_tweets):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print \"Review %d of %d\\n\" % ( i+1, num_tweets)                                                                    \n",
    "    clean_tweets.append(tweets_to_words(tweets['text'][i],\"L\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(clean_tweets, columns=['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('final_cleaned_tweets_new.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_word = 'besigye' # pass a term\n",
    "search_museveni = 'museveni'\n",
    "search_kiggundu = 'kiggundu'\n",
    "count_search = Counter()\n",
    "count_museveni = Counter()\n",
    "count_kiggundu = Counter()\n",
    "count_all = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Print the first 5 most frequent words\n",
      "[('uganda', 1335), ('election', 1298), ('voting', 1246), ('polling', 1164), ('station', 1118), ('medium', 934), ('social', 885), ('besigye', 883), ('ugandan', 835), ('museveni', 748), ('ballot', 711), ('u', 701), ('result', 674), ('people', 668), ('time', 564), ('material', 557), ('day', 485), ('pm', 449), ('today', 438), ('voter', 430), ('ec', 422), ('arrested', 421), ('still', 420), ('2', 417), ('like', 412), ('police', 393), ('president', 374), ('m7', 368), ('say', 354), ('4', 350), (\"it's\", 343), ('paper', 322), ('candidate', 322), ('vpn', 320), ('twitter', 311), ('yet', 306), ('blocked', 297), ('get', 295), ('delay', 289), (\"don't\", 275)]\n",
      "\n",
      " Print the first 5 most frequent hashtags\n",
      "[('#ugandadecides', 14282), ('#ugandaelections', 751), ('#uganda', 395), ('#musevenidecides', 368), ('#ivoted', 138), ('#wesigebesigye', 96), ('#socialmedia', 80), ('#besigye', 78), ('#besigyewon', 78), ('#ipledgepeaceug', 78), ('#ugdecides', 66), ('#museveni', 64), ('#', 59), ('#vpn', 57), ('#kampala', 57), ('#tnr', 51), ('#topowa', 50), ('#ugandavotes', 46), ('#daybreakhitz', 45), ('#peacefirst', 45), ('#rysenshyne', 44), ('#throwbackmusic', 43), ('#provisionalresults', 39), ('#freebesigye', 39), ('#africa', 38), ('#kiggunduresignnow', 36), ('#whytourkenya2016', 32), ('#ichoosepeaceug', 32), ('#electiondayug', 30), ('#kot', 28), ('#election2016', 27), ('#ec', 26), (\"#uganda's\", 24), ('#ugandans', 23), ('#facebook', 22), ('#elections', 22), ('#m7', 22), ('#ktninabamba', 21), ('#twitter', 20), ('#ucc', 20)]\n",
      "\n",
      " Get the most frequent co-occurrences\n",
      "[(('polling', 'station'), 1012), (('medium', 'social'), 881), (('material', 'voting'), 366), (('ballot', 'paper'), 345), (('election', 'uganda'), 297), (('station', 'voting'), 281), (('polling', 'voting'), 261), (('arrested', 'besigye'), 256), (('besigye', 'kizza'), 239), (('pm', 'voting'), 230), (('ballot', 'box'), 215), (('7', 'pm'), 213), (('material', 'polling'), 212), (('besigye', 'museveni'), 184), (('provisional', 'result'), 183), (('election', 'peaceful'), 183), (('4', 'pm'), 180), (('7', 'voting'), 172), (('material', 'station'), 167), (('arrested', 'kizza'), 164)]\n",
      "\n",
      "\n",
      "\n",
      "Co-occurrence for besigye:\n",
      "[('besigye', 883), ('arrested', 251), ('kizza', 236), ('museveni', 175), ('police', 117), ('station', 97), ('polling', 82), ('presidential', 78), ('result', 72), ('dr', 71), ('naguru', 70), ('m7', 68), ('candidate', 65), ('uganda', 65), ('released', 59), ('opposition', 56), ('challenger', 49), ('election', 48), ('rigging', 47), ('win', 44), ('house', 41), ('arrest', 40), ('provisional', 39), ('home', 39), ('0', 37), ('president', 36), ('others', 35), ('time', 34), ('2', 31), ('say', 31), ('ugandan', 30), ('leader', 28), ('amama', 25), ('1', 24), ('going', 24), ('get', 24), ('mbabazi', 24), ('rukungiri', 22), ('lead', 21), ('got', 20)]\n",
      "\n",
      "\n",
      "\n",
      "Co-occurrence for museveni:\n",
      "[('museveni', 748), ('besigye', 173), ('uganda', 154), ('president', 145), ('yoweri', 88), ('election', 81), ('seat', 79), ('retain', 78), ('result', 64), ('win', 54), ('ugandan', 52), ('kaguta', 50), ('station', 48), ('medium', 45), ('social', 41), ('polling', 41), ('others', 40), ('provisional', 39), ('0', 38), ('2', 37), ('race', 33), ('28', 32), ('voting', 28), ('people', 28), ('1', 26), ('time', 26), ('4', 25), ('tight', 25), ('kizza', 25), ('3', 24), ('candidate', 23), ('power', 23), ('mbabazi', 22), ('rest', 22), ('still', 21), ('already', 20), ('term', 20), ('year', 20), ('like', 19), ('amama', 18)]\n",
      "\n",
      "\n",
      "\n",
      "Co-occurrence for kiggundu:\n",
      "[('kiggundu', 203), ('pm', 43), ('badru', 40), ('voting', 30), ('say', 24), ('7', 23), ('polling', 21), ('ec', 20), ('time', 20), ('material', 19), ('ballot', 18), ('delay', 17), ('4', 17), ('extended', 17), ('station', 16), ('dr', 15), ('result', 15), ('kampala', 14), ('eng', 13), ('election', 13), ('want', 12), ('ugandan', 12), ('wakiso', 11), ('come', 11), ('paper', 10), ('00', 10), ('people', 10), ('delayed', 9), ('delivery', 9), ('apology', 9), ('u', 9), ('failed', 8), ('today', 7), ('part', 7), ('due', 7), (\"ec's\", 7), ('guy', 6), ('calm', 6), ('challenge', 6), ('truck', 6)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:30: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:36: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "# remember to include the other import from the previous post\n",
    "com = defaultdict(lambda : defaultdict(int))\n",
    "#We could also look for a specific term and extract its most frequent co-occurrences.\n",
    "\n",
    "reader = csv.reader(open('final_cleaned_tweets_new.csv'), delimiter=\",\")\n",
    "tweet_data = []\n",
    "for row in reader:\n",
    "    tweet_data.append(row[1])\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via','\\xe2','\\x80', '\\xa6','@nbstv','\\xed', 'RT', '\\xa0','\\xbd','I'\n",
    "                                                  ,'amp','We','says','\\x82', '\\xb8', '\\x8a', '\\xbc', '\\xbe','\\x8c', '\\x9c', ':/'\n",
    "                                                  ,'know','The','https','11','78','96','If','one','go','\\xad','\\xad', \n",
    "                                                   '\\xc2','\\xc2', '\\xcb','\\xc2', '\\xc3','\\xae', '\\xc2','\\xad', '\\xcb',\n",
    "                                                   '\\xaa', '\\xc2','\\xad', '\\xc3','\\xa2', '\\xc2','\\xad', '\\xae','\\xc2', \n",
    "                                                   '\\xc5','\\xad', '\\xc5','\\xa4', '\\xc2','\\xaa', '\\xad','\\x98', '\\xc2',\n",
    "                                                   '\\xc3', '\\xc5','\\xa2', '\\xad', '\\xc3', '\\xcb', '\\xb9', '\\xc2', '\\xae', '\\xcb', \n",
    "                                                   '\\xb6', '\\xc2','\\xad','\\xc2','\\xc3', '\\xc5','\\xbf','\\x9e', '\\xb0','\\xab', '\\xb0',\n",
    "                                                   '\\x8f', '\\xb0','\\x9d', '\\xb0','\\x8d', '\\xb0','\\x81', '\\xb0','vote', '\\xb0', '\\xb0', '\\xb0']\n",
    "\n",
    "count_terms = Counter()\n",
    "tweet_data[3]\n",
    "toks = []\n",
    "for i in range(len(tweet_data)):\n",
    "#     tokens = preprocess(tweet_data[i])\n",
    "#     toks.append(tokens)\n",
    "#     terms_all = [term for term in preprocess(tweet_data[i])]\n",
    "    terms_all = [term.lower() for term in preprocess(tweet_data[i]) if term not in stop and not term.startswith(('#', '@'))] \n",
    "    # Count terms only once, equivalent to Document Frequency\n",
    "    terms_single = set(terms_all)\n",
    "    # Count hashtags only\n",
    "    terms_hash = [term.lower() for term in preprocess(tweet_data[i]) if term.startswith('#')]\n",
    "    # Count terms only (no hashtags, no mentions)\n",
    "    terms_only = [term.lower() for term in preprocess(tweet_data[i]) if term not in stop and not term.startswith(('#', '@'))] \n",
    "    # mind the ((double brackets))\n",
    "    # startswith() takes a tuple (not a list) if \n",
    "    # we pass a list of inputs\n",
    "        # Update the counter\n",
    "    \n",
    "    # Build co-occurrence matrix\n",
    "    for i in range(len(terms_only)-1):            \n",
    "        for j in range(i+1, len(terms_only)):\n",
    "            w1, w2 = sorted([terms_only[i], terms_only[j]])                \n",
    "            if w1 != w2:\n",
    "                com[w1][w2] += 1\n",
    "#We could also look for a specific term and extract its most frequent co-occurrences.\n",
    "    if search_word in terms_only:\n",
    "            count_search.update(terms_only)\n",
    "    if search_museveni in terms_only:\n",
    "        count_museveni.update(terms_only)\n",
    "    if search_kiggundu in terms_only:\n",
    "            count_kiggundu.update(terms_only)\n",
    "\n",
    "    count_all.update(terms_all)\n",
    "    count_terms.update(terms_hash)\n",
    "#toks\n",
    "# Print the first 5 most frequent words\n",
    "print '\\n Print the first 5 most frequent words'\n",
    "print count_all.most_common(40)\n",
    "print '\\n Print the first 5 most frequent hashtags'\n",
    "print count_terms.most_common(40)\n",
    "\n",
    "\n",
    "com_max = []\n",
    "# For each term, look for the most common co-occurrent terms\n",
    "for t1 in com:\n",
    "    t1_max_terms = sorted(com[t1].items(), key=operator.itemgetter(1), reverse=True)[:5]\n",
    "    for t2, t2_count in t1_max_terms:\n",
    "        com_max.append(((t1, t2), t2_count))\n",
    "# Get the most frequent co-occurrences\n",
    "terms_max = sorted(com_max, key=operator.itemgetter(1), reverse=True)\n",
    "print '\\n Get the most frequent co-occurrences'\n",
    "print(terms_max[:20])\n",
    "\n",
    "print '\\n\\n'\n",
    "print(\"Co-occurrence for %s:\" % search_word)\n",
    "print(count_search.most_common(40))\n",
    "print '\\n\\n'\n",
    "print(\"Co-occurrence for %s:\" % search_museveni)\n",
    "print(count_museveni.most_common(40))\n",
    "print '\\n\\n'\n",
    "print(\"Co-occurrence for %s:\" % search_kiggundu)\n",
    "print(count_kiggundu.most_common(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondly, our first plot. Using the list of most frequent terms (without hashtags) \n",
    "#### from our data set, we want to plot their frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis is one of the interesting applications of text analytics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('uganda', 1335), ('election', 1298), ('voting', 1246), ('polling', 1164), ('station', 1118), ('medium', 934), ('social', 885), ('besigye', 883), ('ugandan', 835), ('museveni', 748), ('ballot', 711), ('u', 701), ('result', 674), ('people', 668), ('time', 564), ('material', 557), ('day', 485), ('pm', 449), ('today', 438), ('voter', 430)]\n"
     ]
    }
   ],
   "source": [
    "count_top = count_all.most_common(20)\n",
    "print count_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('polling', 'station'), 1012), (('medium', 'social'), 881), (('material', 'voting'), 366), (('ballot', 'paper'), 345), (('election', 'uganda'), 297), (('station', 'voting'), 281), (('polling', 'voting'), 261), (('arrested', 'besigye'), 256), (('besigye', 'kizza'), 239), (('pm', 'voting'), 230), (('ballot', 'box'), 215), (('7', 'pm'), 213), (('material', 'polling'), 212), (('besigye', 'museveni'), 184), (('provisional', 'result'), 183), (('election', 'peaceful'), 183), (('4', 'pm'), 180), (('7', 'voting'), 172), (('material', 'station'), 167), (('arrested', 'kizza'), 164), (('day', 'election'), 164), (('time', 'voting'), 161), (('medium', 'uganda'), 161), (('social', 'uganda'), 158), (('museveni', 'uganda'), 157), (('museveni', 'president'), 145), (('mobile', 'money'), 144), (('facebook', 'twitter'), 143), (('blocked', 'social'), 141), (('blocked', 'medium'), 140), (('delay', 'voting'), 132), (('election', 'ugandan'), 132), (('election', 'fair'), 131), (('voting', 'yet'), 126), (('election', 'medium'), 124), (('ballot', 'station'), 121), (('president', 'uganda'), 121), (('pm', 'polling'), 120), (('besigye', 'police'), 117), (('fair', 'free'), 115), (('commission', 'electoral'), 112), (('ballot', 'polling'), 111), (('pm', 'station'), 104), (('facebook', 'whatsapp'), 103), (('besigye', 'station'), 102), (('pm', 'time'), 101), (('ballot', 'voting'), 99), (('polling', 'voter'), 98), (('kampala', 'voting'), 97), (('extended', 'pm'), 97), (('medium', 'ugandan'), 97), (('shut', 'social'), 95), (('started', 'voting'), 94), (('social', 'ugandan'), 94), (('block', 'social'), 94), (('candidate', 'presidential'), 92), (('polling', 'yet'), 92), (('extended', 'voting'), 91), (('museveni', 'yoweri'), 89), (('block', 'medium'), 89), (('7', 'extended'), 88), (('medium', 'shut'), 88), (('besigye', 'polling'), 87), (('station', 'voter'), 86), (('delayed', 'voting'), 86), (('polling', 'time'), 86), (('voter', 'voting'), 84), (('arrested', 'opposition'), 84), (('still', 'voting'), 83), (('arrested', 'naguru'), 83), (('result', 'station'), 82), (('4', 'voting'), 82), (('late', 'voting'), 82), (('access', 'social'), 82), (('today', 'uganda'), 82), (('station', 'yet'), 81), (('twitter', 'uganda'), 81), (('twitter', 'whatsapp'), 80), (('facebook', 'uganda'), 79), (('president', 'seat'), 79), (('uganda', 'voting'), 79), (('museveni', 'seat'), 79), (('access', 'medium'), 78), (('museveni', 'retain'), 78), (('seat', 'uganda'), 77), (('day', 'uganda'), 77), (('leader', 'opposition'), 77), (('day', 'social'), 76), (('retain', 'seat'), 76), (('people', 'uganda'), 75), (('president', 'retain'), 75), (('day', 'medium'), 75), (('retain', 'uganda'), 75), (('medium', 'ucc'), 75), (('people', 'voting'), 73), (('material', 'time'), 73), (('people', 'polling'), 72), (('2', 'station'), 72), (('kizza', 'presidential'), 72), (('kb', 'm7'), 72)]\n"
     ]
    }
   ],
   "source": [
    "terms_top = terms_max[:100]\n",
    "print terms_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_docs = len(count_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16350"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# n_docs is the total n. of tweets\n",
    "p_t = {}\n",
    "p_t_com = defaultdict(lambda : defaultdict(int))\n",
    "\n",
    "i = 0\n",
    "\n",
    "if i < 20:\n",
    "    for term, n in count_all.items():\n",
    "        p_t[term] = n / n_docs\n",
    "        for t2 in com[term]:\n",
    "            p_t_com[term][t2] = com[term][t2] / n_docs\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_words(words_file):\n",
    "    return [word for line in open(words_file, 'r') for word in line.split()]\n",
    "positive_vocab = read_words('positive-words-edit.txt')\n",
    "negative_vocab = read_words('negative-words-edit.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "pmi = defaultdict(lambda : defaultdict(int))\n",
    "\n",
    "if i < 20:\n",
    "    for t1 in p_t:\n",
    "        for t2 in com[t1]:\n",
    "            denom = p_t[t1] * p_t[t2]\n",
    "            pmi[t1][t2] = math.log((p_t_com[t1][t2] / denom),2)\n",
    "    i = i+1\n",
    " \n",
    "semantic_orientation = {}\n",
    "p = 0\n",
    "if p < 20:\n",
    "    for term, n in p_t.items():\n",
    "        positive_assoc = sum(pmi[term][tx] for tx in positive_vocab)\n",
    "        negative_assoc = sum(pmi[term][tx] for tx in negative_vocab)\n",
    "        semantic_orientation[term] = positive_assoc - negative_assoc\n",
    "    p = p+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can print out the semantic orientation for some terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top Positive Terms\n",
      "[('church', 40.73243619920966), ('friend', 37.81271437162385), ('article', 36.99788432309667), ('crossed', 36.773402613852475), ('enjoy', 36.60466873498684), ('gave', 35.19359840488293), ('economic', 34.787376579081254), ('ensuring', 33.258736638037135), ('heart', 31.91018642123106), ('god', 30.958631786321995), ('https://t.co/uj96xhqorp', 30.940075327285683), ('apwoyo', 30.940075327285683), ('humbling', 30.940075327285683), ('23', 30.45662736374392), ('bos', 28.85452801032922), ('col', 28.835085053412946), ('2021', 28.571096478517358), ('anything', 27.796474100244964), ('condition', 27.77408957855264), ('addition', 27.360080315589723), ('apac', 26.132720405228078), ('ba', 26.08376778801862), ('https://t.co/s2pc6cfml0', 25.294777163337056), ('https://t.co/vhfh11mhhf', 25.086453871536854), ('kisoro', 24.891616988752922), ('build', 24.625811012020034), ('https://t.co/7zmgp00848', 24.50336378514847), ('god-like', 24.398823799980583), ('kotido', 24.1164750750382), ('https://t.co/twfwddkdic', 23.81386129925943), ('enuf', 23.81386129925943), ('expect', 23.695848280055596), ('analyst', 23.624584063897046), ('contender', 23.59962432666652), ('membered', 23.45733458363368), ('dwell', 23.378634623030166), ('glad', 23.356746943440847), ('either', 23.18687423987993), ('began', 23.133306635981704), ('conducted', 23.075120377665723), ('heat', 23.062929257310216), ('bless', 22.974206588597383), ('mafabi', 22.971792535966813), ('nandala', 22.971792535966813), ('nathan', 22.971792535966813), ('https://t.co/yuhwscwy0b', 22.955068327194674), ('continue', 22.91938821881775), ('bother', 22.753152249491475), ('constitution', 22.645560755745766), ('inclusion', 22.386945255742283), ('abandoned', 22.19430017779989), ('ele', 22.149433791173493), ('https://t.co/afik', 21.971907756463438), ('https://t.co/afikuy5ezl', 21.971907756463438), ('concede', 21.795036330527783), ('https://t.co/eh1frnbgrm', 21.730025023877708), ('https://t.co/wbalqpvqrd', 21.649979661576076), ('atmosphere', 21.60933767707873), ('done', 21.56559246745297), ('argued', 21.493860459658794), ('co-existence', 21.331566369373387), (\"country's\", 21.186942806293935), ('coalition', 21.024375176357573), ('minister', 20.973904620951398), ('cheering', 20.965481487304004), ('https://t.co/z2ud5aayyf', 20.884453952503975), ('contest', 20.87049129946593), ('neighbour', 20.86492277771851), ('electio', 20.851097214664403), ('loyalty', 20.70557727221745), ('hawker', 20.70244708147021), ('defeat', 20.579742247523928), ('39', 20.55044398802516), ('2014,', 20.55044398802516), ('https://t.co/dzo5', 20.55044398802516), ('https://t.co/fa4nmjn2v9', 20.55044398802516), ('buvuma', 20.55044398802516), ('https://t.co/dzo5b8hx3y', 20.55044398802516), ('2011', 20.508531795658683), ('jammed', 20.44479989265386), ('15', 20.393807455054194), ('initially', 20.25570072246403), ('nation', 20.22606138111488), ('initiative', 20.19115125020355), ('blood', 20.188910584850777), ('encourage', 20.184400705680005), ('au', 20.129442502785572), ('obvious', 19.967672667239885), ('defy', 19.962129593389598), ('stability', 19.9506486047782), ('clinton', 19.872372082912527), ('hillary', 19.872372082912527), ('https://t.co/isk8io4pmi', 19.872372082912527), ('plus', 19.75373778229043), ('https://t.co/00gqjnpgqh', 19.70244708147021), ('https://t.co/lahmij9zwm', 19.70244708147021), ('imbecile', 19.60933767707873), ('advertise', 19.60933767707873), ('https://t.co/e', 19.51912824137909), ('heaven', 19.495118210497775)]\n",
      "\n",
      "The Top Negative Terms\n",
      "[('africa', -36.85503097483728), ('hated-branson', -37.16889401778157), ('emotion', -37.16889401778157), ('beat', -37.333725034862695), ('apprently', -37.361539095723955), ('chaos', -37.62983222443414), ('m7', -37.81704688571204), ('delicate', -38.03692491731836), ('nrm', -38.096497705547286), ('kay', -38.27419056538457), ('act', -38.85330499957179), ('abt', -38.92990787377345), ('careful', -39.1374609942263), ('http', -39.20997221936959), ('call', -39.28150632702659), ('effort', -39.54695214710349), ('bullet', -40.30637482577057), ('deal', -40.53749485841647), (\"aren't\", -40.6743343683319), ('even', -40.88884054767502), ('better', -41.413428375875526), ('many', -41.456030650377116), ('killed', -41.47153272210158), ('rally', -41.519696470198156), ('child', -41.57400473756246), ('elephant', -41.621785485151584), ('cause', -42.80626921820925), ('ec', -43.09487118811473), ('1', -43.75894169805016), ('history', -44.87316946841303), ('around', -45.209613875400656), ('control', -45.27888710672596), ('country', -45.97873758486801), ('bomb', -47.131814051897116), ('kayihura', -47.780311922670556), ('camera', -48.173476250694975), ('besigye', -49.405135497477424), ('arrested', -50.1117841815226), ('like', -53.32271215334261), ('creating', -54.273919102588025), ('blast', -54.879948626847266), ('electoral', -55.03453110836194), ('4', -55.862406400178024), ('2', -58.98562773130945), ('people', -63.364154211341884), ('ballot', -67.01181065001545), ('arrest', -70.615217200883), (\"it's\", -71.64797282648473), ('change', -77.64789594834171), (\"don't\", -79.19950832292027)]\n"
     ]
    }
   ],
   "source": [
    "semantic_sorted = sorted(semantic_orientation.items(), \n",
    "                         key=operator.itemgetter(1), \n",
    "                         reverse=True)\n",
    "top_pos = semantic_sorted[:100]\n",
    "top_neg = semantic_sorted[-50:]\n",
    "\n",
    "print 'The Top Positive Terms'\n",
    "print(top_pos)\n",
    "print '\\nThe Top Negative Terms'\n",
    "print(top_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "besigye: -49.405135\n",
      "museveni: -32.226432\n",
      "kiggundu: -16.960529\n"
     ]
    }
   ],
   "source": [
    "print(\"besigye: %f\" % semantic_orientation['besigye'])\n",
    "print(\"museveni: %f\" % semantic_orientation['museveni'])\n",
    "print(\"kiggundu: %f\" % semantic_orientation['kiggundu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-32.226431875358955, -27.984023654857886)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-60.21045553021684"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1 = 'museveni'\n",
    "word2 = 'president'\n",
    "print(semantic_orientation[word1], \n",
    "      semantic_orientation[word2]) \n",
    "semantic_orientation[word1] + \\\n",
    "semantic_orientation[word2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10732"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(semantic_orientation)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
